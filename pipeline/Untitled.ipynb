{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                         ...Connecting to the database...                        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import *\n",
    "from db_config import *\n",
    "session,engine = connect_db()\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine,Column,Integer,String,ForeignKey,Table,Text,inspect\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker,relationship\n",
    "\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "font = {'size'   : 21}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for build in session.query(Build).filter(Build.status == 'building').order_by(Build.build_name):\n",
    "    print(build.build_name,build.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now(),'Began run')\n",
    "\n",
    "query_outcomes = \"SELECT parts.part_id,parts.status,wells.seq_outcome,wells.plate_type,builds.build_name,wells.misplaced FROM parts \\\n",
    "        INNER JOIN wells ON parts.id = wells.part_id\\\n",
    "        INNER JOIN plates ON wells.plate_id = plates.id\\\n",
    "        INNER JOIN builds ON plates.build_id = builds.id\"\n",
    "\n",
    "query_frag = \"SELECT parts.part_id,fragments.fragment_name,twist_orders.sub_name FROM parts\\\n",
    "        INNER JOIN part_frag ON parts.id = part_frag.part_id\\\n",
    "        INNER JOIN fragments ON part_frag.fragment_id = fragments.id\\\n",
    "        INNER JOIN frag_order ON fragments.id = frag_order.frag_id\\\n",
    "        INNER JOIN twist_orders ON twist_orders.id = frag_order.twist_order_id\"\n",
    "\n",
    "query_parts = \"SELECT * FROM parts\"\n",
    "\n",
    "df_frag = pd.read_sql_query(query_frag, con=engine)\n",
    "frags = df_frag.groupby('part_id')['fragment_name'].agg(len)\n",
    "frags.name = 'Count'\n",
    "frags = pd.DataFrame(frags).reset_index()\n",
    "frags_dict = dict(zip(frags.part_id.tolist(),frags.Count.tolist()))\n",
    "subs_dict = dict(zip(df_frag.part_id.tolist(),df_frag.sub_name.tolist()))\n",
    "\n",
    "author_dict = []\n",
    "for file in sorted(glob.glob('../data/*/*.json')):\n",
    "    with open(file,\"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    author_dict.append([data['gene_id'],data['author']['name']])\n",
    "author_dict = dict(author_dict)\n",
    "\n",
    "print(datetime.now(),'Finished frags')\n",
    "\n",
    "def multiple(x):\n",
    "    if len(x) == 1:\n",
    "        x.append('N/A')\n",
    "    return x\n",
    "\n",
    "def find_outcome(x):\n",
    "    if x in df_out_dict.keys():\n",
    "        return df_out_dict[x]\n",
    "    else:\n",
    "        return ['N/A','N/A']\n",
    "    \n",
    "def find_build(x):\n",
    "    if x in df_build_dict.keys():\n",
    "        return df_build_dict[x]\n",
    "    else:\n",
    "        return ['N/A','N/A']\n",
    "    \n",
    "def simplify_outcome(x):\n",
    "    if \"mutation\" in x:\n",
    "        return 'cloning_mutation'\n",
    "    elif \"bad\" in x:\n",
    "        return 'sequence_failure'\n",
    "#     elif x == 'cloning_error':\n",
    "#         return 'cloning_failure'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def find_author(x):\n",
    "    return author_dict[x]\n",
    "\n",
    "df_res = pd.read_sql_query(query_outcomes, con=engine)\n",
    "df_res = df_res[df_res.plate_type == 'seq_plate']\n",
    "\n",
    "df_out = df_res.groupby('part_id')['seq_outcome'].apply(list)\n",
    "df_out.name = 'Outcomes'\n",
    "df_out = pd.DataFrame(df_out).reset_index()\n",
    "df_out.Outcomes = df_out.Outcomes.apply(multiple)\n",
    "df_out_dict = dict(zip(df_out.part_id.tolist(),df_out.Outcomes.tolist()))\n",
    "\n",
    "df_build = df_res.groupby('part_id')['build_name'].apply(list)\n",
    "df_build.name = 'Builds'\n",
    "df_build = pd.DataFrame(df_build).reset_index()\n",
    "df_build.Builds = df_build.Builds.apply(multiple)\n",
    "df_build_dict = dict(zip(df_build.part_id.tolist(),df_build.Builds.tolist()))\n",
    "print(datetime.now(),'Finished outcomes')\n",
    "\n",
    "df_parts = pd.read_sql_query(query_parts, con=engine)\n",
    "print('finished part query')\n",
    "\n",
    "df_parts = df_parts[df_parts.part_id != 'BBF10K_000745']\n",
    "\n",
    "df_parts['Fragments'] = df_parts.part_id.apply(lambda x: frags_dict[x])\n",
    "df_parts['Submission'] = df_parts.part_id.apply(lambda x: subs_dict[x])\n",
    "df_parts['Order_number'] = df_parts.Submission.apply(lambda name: int(name[-3:]))\n",
    "df_parts['Outcomes'] = df_parts.part_id.apply(find_outcome)\n",
    "df_parts['Builds'] = df_parts.part_id.apply(find_build)\n",
    "print('finished outcome and builds')\n",
    "df_parts['Attempt_1_Outcome'] = df_parts.Outcomes.apply(lambda x: x[0])\n",
    "df_parts['Attempt_1_Outcome_G'] = df_parts.Attempt_1_Outcome.apply(simplify_outcome)\n",
    "df_parts['Attempt_1_Build'] = df_parts.Builds.apply(lambda x: x[0])\n",
    "df_parts['Attempt_2_Outcome'] = df_parts.Outcomes.apply(lambda x: x[1])\n",
    "df_parts['Attempt_2_Outcome_G'] = df_parts.Attempt_2_Outcome.apply(simplify_outcome)\n",
    "df_parts['Attempt_2_Build'] = df_parts.Builds.apply(lambda x: x[1])\n",
    "df_parts['Length'] = df_parts.seq.apply(len)\n",
    "df_parts['Author'] = df_parts.part_id.apply(find_author)\n",
    "print(datetime.now(),'Finished building dataframe')\n",
    "df_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for build in session.query(Build).filter(Build.build_name == 'build029'):\n",
    "    for plate in build.plates:\n",
    "        if plate.plate_type != 'assembly_plate':\n",
    "            continue\n",
    "        for well in plate.wells:\n",
    "            print(well.parts.part_id,well.address,well.parts.status)\n",
    "#             well.parts.status = 'building'\n",
    "# session.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[1,2],[3,4],[5,6]]\n",
    "# while len(test) > 0:\n",
    "num = test.pop()\n",
    "print(num,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parts.status.value_counts()\n",
    "df_bt = df_parts[df_parts.Fragments > 1]\n",
    "# df_bt = df_bt[df_bt.Submission == 'submission010']\n",
    "df_bt\n",
    "# df_bt.Submission.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_parts.copy()\n",
    "\n",
    "# df_parts.status.value_counts()\n",
    "# df_ordered = df_parts[df_parts.status == 'synthesis_abandoned']\n",
    "# df_ordered.Order_number.value_counts()\n",
    "\n",
    "# df_parts = df_parts[df_parts.status == 'received']\n",
    "df = df[df.part_id == 'BBF10K_001343']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for build in session.query(Build).order_by(Build.build_name).filter(Build.build_name == 'build029'):\n",
    "    print(build.build_name)\n",
    "    print(len(build.plates[0].wells))\n",
    "    for plate in build.plates:\n",
    "        print(plate.plate_name,plate.plate_type)\n",
    "        for well in plate.wells:\n",
    "            print(well.parts.part_id,well.address)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builds = range(16,31)\n",
    "builds = ['build'+str(build).zfill(3) for build in builds]\n",
    "print(builds)\n",
    "\n",
    "\n",
    "query_builds = \"SELECT parts.part_id,parts.status,wells.plate_type,builds.build_name,plates.id FROM parts \\\n",
    "        INNER JOIN wells ON parts.id = wells.part_id\\\n",
    "        INNER JOIN plates ON wells.plate_id = plates.id\\\n",
    "        INNER JOIN builds ON plates.build_id = builds.id\"\n",
    "\n",
    "df_builds = pd.read_sql_query(query_builds, con=engine)\n",
    "df_builds.build_name.value_counts()\n",
    "df_assembly = df_builds[df_builds.build_name.isin(builds)]\n",
    "\n",
    "# df_assembly.status.value_counts()\n",
    "# print(len(df_assembly.part_id.tolist()))\n",
    "# print(len(df_assembly.part_id.unique().tolist()))\n",
    "df_ids = df_assembly[['build_name','id']].drop_duplicates()\n",
    "df_ids\n",
    "# df_assembly = df_builds[df_builds.plate_type == 'assembly_plate']\n",
    "# df_building = df_assembly[df_assembly.status == 'building']\n",
    "# df_building.build_name.value_counts()\n",
    "\n",
    "dups = pd.concat(g for _, g in df_assembly.groupby(\"part_id\") if len(g) > 1)\n",
    "dups.build_name.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_build = 'build022'\n",
    "query = \"SELECT parts.part_id,builds.build_name,part_wells.address as destination,fragments.fragment_name,frag_plates.plate_name,frag_plates.plate_id,frag_wells.address as source,frag_wells.volume FROM parts \\\n",
    "        INNER JOIN wells AS part_wells ON parts.id = part_wells.part_id\\\n",
    "        INNER JOIN plates AS part_plates ON part_wells.plate_id = part_plates.id\\\n",
    "        INNER JOIN builds ON part_plates.build_id = builds.id\\\n",
    "        INNER JOIN part_frag ON parts.id = part_frag.part_id\\\n",
    "        INNER JOIN fragments ON part_frag.fragment_id = fragments.id\\\n",
    "        INNER JOIN wells AS frag_wells ON fragments.id = frag_wells.fragment_id\\\n",
    "        INNER JOIN plates AS frag_plates ON frag_wells.plate_id = frag_plates.id\\\n",
    "        WHERE builds.build_name = '{}'\".format(target_build)\n",
    "\n",
    "df = pd.read_sql_query(query,con=engine)\n",
    "\n",
    "unique_frag = df[['part_id','fragment_name','destination']].drop_duplicates()\n",
    "unique_frag\n",
    "\n",
    "frag_df = unique_frag.groupby('destination').agg(len).part_id\n",
    "frag_df = frag_df.reset_index()\n",
    "frag_df = frag_df.rename(columns={'part_id':'frag_num'})\n",
    "frag_dict = dict(zip(frag_df.destination.tolist(),frag_df.frag_num.tolist()))\n",
    "\n",
    "df['frag_num'] = df.destination.apply(lambda x: frag_dict[x])\n",
    "df\n",
    "\n",
    "unique_df = df[['part_id','destination','frag_num']].drop_duplicates()\n",
    "unique_df\n",
    "\n",
    "need_extra = unique_df[unique_df.frag_num > 1]\n",
    "need_extra\n",
    "\n",
    "num_reactions = len(df.part_id.unique().tolist())\n",
    "num_reactions\n",
    "# total = unique_df.frag_num.sum()\n",
    "# print(total)\n",
    "\n",
    "\n",
    "# unique_plates = ['syn_plate021','syn_plate025']\n",
    "# query_resuspend = \"SELECT plates.plate_id,plates.resuspended FROM plates\\\n",
    "#                     WHERE plates.resuspended = 'not_resuspended'\\\n",
    "#                         AND plates.plate_id IN ({})\".format(ot.list_to_string(unique_plates))\n",
    "# resuspended = pd.read_sql_query(query_resuspend,con=engine)\n",
    "# resuspended\n",
    "# if len(resuspended) == 0:\n",
    "#     print('all resuspended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for build in session.query(Build).filter(Build.build_name == 'build033'):\n",
    "    print(build.build_name,build.status)\n",
    "    for well in build.plates[0].wells:\n",
    "        print(well.parts.part_id,well.address,well.parts.status)\n",
    "#         well.parts.status = 'received'\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_parts.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = df_parts[df_parts.status == 'received']\n",
    "df_rec.Fragments.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in session.query(Part).filter(Part.status == 'abandoned'):\n",
    "    print(part.part_id,part.status)\n",
    "#     part.status = 'synthesis_abandoned'\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_parts.copy()\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts = df_parts[df_parts.Attempt_1_Build.isin(['build012','build013','build014'])]\n",
    "attempted = ['received','synthesis_abandoned']\n",
    "cloned = ['sequence_confirmed','cloning_mutation']\n",
    "total_bp_ordered = df_parts.Length.sum()\n",
    "total_received = df_parts[df_parts.status != 'synthesis_abandoned'].Length.sum()\n",
    "total_attempted = df_parts[~df_parts.status.isin(attempted)].Length.sum()\n",
    "total_cloned = df_parts[df_parts.status.isin(cloned)].Length.sum()\n",
    "total_perfect = df_parts[df_parts.status == 'sequence_confirmed'].Length.sum()\n",
    "total_mutant = df_parts[df_parts.status == 'cloning_mutation'].Length.sum()\n",
    "\n",
    "\n",
    "# df_parts\n",
    "print(total_bp_ordered)\n",
    "print(total_received)\n",
    "print(total_attempted)\n",
    "print(total_cloned)\n",
    "print(total_cloned/total_attempted)\n",
    "print(total_perfect)\n",
    "print(total_perfect/total_attempted)\n",
    "print(total_mutant)\n",
    "print(total_mutant/total_attempted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempted = ['received','synthesis_abandoned']\n",
    "cloned = ['sequence_confirmed','cloning_mutation']\n",
    "total_bp_ordered = df_parts.Length.sum()\n",
    "total_received = df_parts[df_parts.status != 'synthesis_abandoned'].Length.sum()\n",
    "total_attempted = df_parts[~df_parts.status.isin(attempted)].Length.sum()\n",
    "total_cloned = df_parts[df_parts.status.isin(cloned)].Length.sum()\n",
    "total_perfect = df_parts[df_parts.status == 'sequence_confirmed'].Length.sum()\n",
    "total_mutant = df_parts[df_parts.status == 'cloning_mutation'].Length.sum()\n",
    "total_gene_cloned = len(df_parts[df_parts.status.isin(cloned)])\n",
    "largest = df_parts[df_parts.status.isin(cloned)].Length.max()\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Ordered':total_bp_ordered,\n",
    "    'Received':total_received,\n",
    "    'Attempted':total_attempted,\n",
    "    'Cloned':total_cloned,\n",
    "    'Perfect':total_perfect,\n",
    "    'Mutations':total_mutant\n",
    "},index=['counts'])\n",
    "\n",
    "# df_parts\n",
    "print(total_bp_ordered)\n",
    "print(total_received)\n",
    "print(total_attempted)\n",
    "print(total_cloned)\n",
    "print(total_cloned/total_attempted)\n",
    "print(total_perfect)\n",
    "print(total_perfect/total_attempted)\n",
    "print(total_mutant)\n",
    "print(total_mutant/total_attempted)\n",
    "print(total_gene_cloned)\n",
    "print(largest)\n",
    "# stats['Percent_cloned'] = stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = '1'\n",
    "print([int(num) for num in ans.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3.424444444\n",
    "new_num = round(num,2)\n",
    "new_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts.groupby('Order_number').Length.agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts.groupby('status').Length.agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for build in session.query(Build).order_by(Build.build_name).filter(Build.build_name == 'build016'):\n",
    "    print(build.build_name)\n",
    "    for plate in build.plates:\n",
    "        for well in plate.wells:\n",
    "            print(well.parts.part_id,well.address,well.parts.status)\n",
    "#             well.parts.status = 'building'\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build = 'build021'\n",
    "# data = pd.read_csv('{}/builds/{}/{}_trans_map.csv'.format(BASE_PATH,build,build))\n",
    "# data\n",
    "# parts = [part for part in session.query(Part).filter(Part.part_id.in_(data.Gene.tolist()))]\n",
    "# target_build = Build(parts,build_name=build)\n",
    "# for i,(gene,well) in data.iterrows():\n",
    "#     print(\"working on: \", gene,well)\n",
    "#     gene_obj = session.query(Part).filter(Part.part_id == gene).one()\n",
    "#     target_build.add_item(gene_obj,well)\n",
    "for plate in session.query(Build).filter(Build.build_name == build).one().plates:\n",
    "    for well in plate.wells:\n",
    "        print(well.parts.part_id,well.address)\n",
    "# input('continue')\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build = 'build023'\n",
    "if len([build for build in session.query(Build).filter(Build.build_name == build)]) == 0:\n",
    "    print('not found')\n",
    "else:\n",
    "    print('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot_functions as ot\n",
    "\n",
    "author_dict = []\n",
    "for file in sorted(glob.glob('../data/*/*.json')):\n",
    "    with open(file,\"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    author_dict.append([data['gene_id'],data['author']['name']])\n",
    "authors = [a for g,a in author_dict]\n",
    "author_dict = dict(author_dict)\n",
    "\n",
    "authors = pd.Series(authors).unique()\n",
    "print('Current authors:\\n',authors)\n",
    "author = ot.request_info('Enter author name: ',select_from=authors)\n",
    "\n",
    "query = \"SELECT parts.part_id,parts.status,builds.build_name,part_wells.address,part_wells.seq_outcome,fragments.fragment_name,frag_plates.plate_name,frag_wells.address FROM parts \\\n",
    "        INNER JOIN wells AS part_wells ON parts.id = part_wells.part_id\\\n",
    "        INNER JOIN plates AS part_plates ON part_wells.plate_id = part_plates.id\\\n",
    "        INNER JOIN builds ON part_plates.build_id = builds.id\\\n",
    "        INNER JOIN part_frag ON parts.id = part_frag.part_id\\\n",
    "        INNER JOIN fragments ON part_frag.fragment_id = fragments.id\\\n",
    "        INNER JOIN wells AS frag_wells ON fragments.id = frag_wells.fragment_id\\\n",
    "        INNER JOIN plates AS frag_plates ON frag_wells.plate_id = frag_plates.id\\\n",
    "        WHERE part_wells.plate_type = 'seq_plate'\"\n",
    "\n",
    "data = pd.read_sql_query(query,con=engine)\n",
    "data['author'] = data.part_id.apply(lambda x: author_dict[x])\n",
    "data = data.sort_values('part_id')\n",
    "data = data[data.author == author]\n",
    "data\n",
    "\n",
    "# author_path = '{}/'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = str(datetime.now()).split(\" \")[0]\n",
    "\n",
    "data = data.sort_values('build_name')\n",
    "data\n",
    "author_path = '{}/authors/{}'.format(BASE_PATH,author)\n",
    "ot.make_directory(author_path)\n",
    "data.to_csv('{}/{}_well_locations_{}'.format(author_path,author,now),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(str(datetime.now()).split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = []\n",
    "if empty:\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine,Column,Integer,String,ForeignKey,Table,Text,inspect,update,select\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker,relationship\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import getch\n",
    "import shutil\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from config import *\n",
    "import ot_functions as ot\n",
    "from db_config import *\n",
    "session,engine = connect_db()\n",
    "conn = engine.connect()\n",
    "    \n",
    "from moclopy import fixer,fragger\n",
    "from synbiolib import codon\n",
    "\n",
    "\n",
    "# query_seqs = \"SELECT parts.part_id,parts.part_name,parts.part_type,parts.original_seq,parts.seq,parts.organism FROM parts\\\n",
    "#                 WHERE parts.seq IS NULL\"\n",
    "\n",
    "query_seqs = \"SELECT parts.part_id,parts.part_name,parts.part_type,parts.original_seq,parts.seq,parts.organism,fragments.fragment_name FROM parts\\\n",
    "                LEFT JOIN part_frag ON parts.id = part_frag.part_id\\\n",
    "                LEFT JOIN fragments ON part_frag.fragment_id = fragments.id\\\n",
    "                WHERE parts.seq IS NOT Null\"\n",
    "\n",
    "df = pd.read_sql_query(query_seqs,con=engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0].original_seq)\n",
    "print(df.loc[0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fragment_sequence(seq,part_type,cloning_enzyme_prefix=\"GAAGACTT\",cloning_enzyme_suffix=\"GCGTCTTC\",synthesis_max=synthesis_max):\n",
    "df['part_type'] = df.part_type.apply(lambda x: 'prokaryotic_promoter' if x == 'prokaryotic promoter' else x)\n",
    "df['fragments'] = df.apply(lambda row: fragger.fragment_sequence(row['seq'],row['part_type']), axis=1)\n",
    "\n",
    "df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize_sequences(row):\n",
    "    if row['part_type'] != 'cds':\n",
    "        return \"not_optimized\"\n",
    "    if row['organism'] == None:\n",
    "        table = codon.load_codon_table(species='ecoli')\n",
    "    else:\n",
    "        table = codon.load_codon_table(species=row['organism'])\n",
    "    protein_seq = fixer.translate(row['original_seq'])\n",
    "    optimized = codon.optimize_protein(table,protein_seq)\n",
    "    \n",
    "    fixed = fixer.fix_sequence(row['part_id'],optimized)\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "print(datetime.now())\n",
    "df = df.loc[0:99,:]\n",
    "df['seq'] = df.apply(optimize_sequences, axis=1)\n",
    "# df['modified'] = df.apply(lambda row: True if row['original_seq'] == row['seq'] else False,axis=1)\n",
    "print(datetime.now())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [part+'_seqs' for part in df.part_id.tolist()]\n",
    "seq_dict = dict(zip(df.part_id.tolist(),seqs))\n",
    "print(seq_dict)\n",
    "\n",
    "for part in session.query(Part).filter(Part.part_id.in_(df.part_id.tolist())):\n",
    "#     part.seq = seq_dict[part.part_id]\n",
    "    part.seq = None\n",
    "\n",
    "\n",
    "# print(seq_dict['FG_001'])\n",
    "\n",
    "# stmt = update(session.query(Part).all()).values(seq='user #5')\n",
    "\n",
    "# conn.execute(stmt)\n",
    "\n",
    "# stmt = select([Part.part_id]).limit(1)\n",
    "# conn.execute(Part.update().values(seq=stmt))\n",
    "\n",
    "\n",
    "# session.query(Part).update({Part.seq:seq_dict[Part.part_id]})\n",
    "session.commit()\n",
    "# session.query(Part).filter(Part.part_id.in_(df.part_id.tolist())).update({Part.seq: 'test'})\n",
    "\n",
    "\n",
    "# df.part_id.tolist()\n",
    "# session.execute(update(parts, values={stuff_table.c.foo: stuff_table.c.foo + 1}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlalchemy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_names = ['build'+str(num).zfill(3) for num in range(11)]\n",
    "build_names\n",
    "for build in session.query(Build).filter(Build.build_name.in_(build_names)).order_by(Build.build_name):\n",
    "    print(build.build_name)\n",
    "    for plate in build.plates:\n",
    "        if plate.plate_type != 'seq_plate':\n",
    "            continue\n",
    "        print(plate.plate_name)\n",
    "        for well in plate.wells:\n",
    "            print(well.vector,well.address)\n",
    "            well.vector = 'popen_v1-1'\n",
    "session.commit()\n",
    "# for well in session.query(Well).join(Plate,Well.plates).join(Build,Plate.builds).filter(Build.build_name.in_(build_names)):\n",
    "#     print(well.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_names = ['build'+str(num).zfill(3) for num in range(30,31)]\n",
    "build_names\n",
    "\n",
    "well_ids = [well.id for well in session.query(Well).join(Plate,Well.plates).join(Build,Plate.builds).filter(Build.build_name.in_(build_names)).filter(Plate.plate_type == 'assembly_plate')]\n",
    "# session.query(Well).join(Plate,Well.plates).join(Build,Plate.builds).filter(Build.build_name.in_(build_names)).filter(Plate.plate_type == 'assembly_plate').update({Well.vector: 'popen_v3-0'})\n",
    "len(well_ids)\n",
    "session.query(Well).filter(Well.id == 23784).update({Well.vector: 'popen_v3-0'})\n",
    "# well_ids\n",
    "# build_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "builds = ['build012','build013','build014']\n",
    "part_ids = []\n",
    "for build in builds:\n",
    "    data = pd.read_csv('{}/builds/{}/{}_seq_plate.csv'.format(BASE_PATH,build,build))\n",
    "    part_ids += data.Part.tolist()\n",
    "print(len(part_ids))\n",
    "\n",
    "query = \"SELECT parts.part_id,fragments.fragment_name,parts.part_type,parts.seq,parts.part_name,parts.cloning_enzyme,fragments.seq FROM parts\\\n",
    "            INNER JOIN part_frag ON parts.id = part_frag.part_id\\\n",
    "            INNER JOIN fragments ON part_frag.fragment_id = fragments.id\\\n",
    "            WHERE parts.part_id IN ({})\".format(ot.list_to_string(part_ids))\n",
    "\n",
    "df = pd.read_sql_query(query, con=engine)\n",
    "\n",
    "df.to_csv('./parts_frags.csv')\n",
    "# for part,df in df.groupby('part_id'):\n",
    "#     print(part)\n",
    "#     for i,row in df.iterrows():\n",
    "#         print(row.fragment_name)\n",
    "        \n",
    "# for part in part_ids:\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
